This is a repo for building a dynamic dependency diagram for all Looker and Snowflake data sources.

# Current worlflow:

1. Script `get_connections.py` contains one function `main(domain, url)` that is ready to be called by passing the recent Looker API endpoint. Secrets are stored in the `.secrets` folder and ignored in repo. The function returns a list of the current LookML connection names, with respective Warehouse names and Database names. 

2. Script `file_separator.py` splits up either views within each explore, or explores within each model, and populates the json files in folder the corresponding `views` folder and `explores` folder.

3. Script `explore_trees.py` reads and parses these json files in the `explores` folder, and return a set of json files that represent the mapping relationships for each explore and its child joins. These mapping files are stored as json files in the `maps` folder. File names in `mapes` folder follow this convention: `explore-[modelName]-[exploreName]`.

4. Script `view_separator.py` splits view files containing multiple view structures into separate view files. Saves single view files under corresponding view folder, `views/view_name`. 

5. Script `view_trees.py` reads the split view files (from step 4), parses the source claused and stores source information based on different types: the direct sourcing from `sql_table_name`, the extension from another view with an overridden `sql_table_name`, the extension from another view without an overridden `sql_table_name`, and extension based on `derived_table` (can be from sql, or explore). The mapping structures in json format are thus saved in `maps` folder, together with other mapping objects. File names in `maps` folder follow this convention: `view-[viewFileName]-[viewName]`.

6. Script `some_script_i_have_not_yet_named.py` calls `get_connections.py` to get an up-to-date connection map. Generates a view mapping based on various types of `view.lkml` parameter, and matches the cloud provider and database names. Currently supporting: `sql_table_name` (relative, and absolute) and `extends`. To be featured: `derived_table` and NDT's.

7. Script `graph_mapping.py` reads explore structure payloads from `maps` folder. For each explore, this script will evaluate the necessary joins, match the join names with view file names, and look for the actual view name that is being called and its underlying tables. Generates graph representations for each explore. These `.gv` files and corresponding `.pdf` files are saved in the folder `graphs`.


# Diagram as of 2019-10-15:
![alt text](dependency.png "Generating Process")

# Sample dependency chart:
![alt text](sample.png "Snowflake Salesforce Explore sf__leads_and_contacts")
![alt text](sf__leads_and_contacts.pdf "Link")

# Ideal goals:

The deliverable will be a static webpage (something like `dbt` does for model lineage visualization), which parses Lookml files from an S3 bucket. The S3 bucket is being updated on a set schedule (hourly/daily), to reflect the near real-time business logics within the Looker ecosystem. 

A tree-shaped diagram indicates all depencencies from the main node (parent node). The data source will be traced back to either a `pdt` table or a raw event table from the databases in Snowflake.

There are two parts of Lookml lineage:

1. Front-end: 
Dashboards, Looks, Explores (renamed by `view_label`), Explore Label groups. Content Validator is the test tool.

2. Back-end:
Models with multiples explores (with original explore name), view names (reference view file names), and folder names. 

On the Snowflake side, all lineages are considered as "Back-end". The destination event tables will be quoted as their real table paths within databases, except for the "persistent derived table (pdt)" that are generated by Looker. In this case, we will extract the human readable pdt name with the `_pdt` suffix, avoiding random strings that are generated at each runtime.

One question to tackle when we move on to Phase two, is that: 

>  How do we stitch together the front-end and back-end lineage for an end-to-end monitoring and tracking process?


# The even bigger picture of this project consists at least the following parts:

1. An end-to-end near real-time tracker system for all data lineage from the raw event sources in the BI databases (Snowflake).

2. Embed a unit testing system based on the main (most popular) queries to GitHub PR, together with content validator.

3. Alerting system for any *yellow light* or *red light* data paths that are experiencing latency, performance issue, or failure at query time.

4. Cost analysis, cost of query per explore, tie back to the team usage of Looker.
